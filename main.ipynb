{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U llama-index-readers-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleDirectoryReader, VectorStoreIndex, Document, SimpleKeywordTableIndex\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceLLM\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Document, SimpleKeywordTableIndex\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 4/4 [06:40<00:00, 100.06s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 1: Set up the HuggingFace LLM\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maaditya/OpenBioLLM-Llama3-8B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maaditya/OpenBioLLM-Llama3-8B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch_dtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m Settings\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m=\u001b[39mllm\n\u001b[1;32m     11\u001b[0m Settings\u001b[38;5;241m.\u001b[39membed_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal:BAAI/bge-small-en-v1.5\u001b[39m\u001b[38;5;124m\"\u001b[39m \n",
      "File \u001b[0;32m~/vishak/miniconda3/envs/lora/lib/python3.10/site-packages/llama_index/llms/huggingface/base.py:237\u001b[0m, in \u001b[0;36mHuggingFaceLLM.__init__\u001b[0;34m(self, context_window, max_new_tokens, query_wrapper_prompt, tokenizer_name, model_name, model, tokenizer, device_map, stopping_ids, tokenizer_kwargs, tokenizer_outputs_to_remove, model_kwargs, generate_kwargs, is_chat_model, callback_manager, system_prompt, messages_to_prompt, completion_to_prompt, pydantic_program_mode, output_parser)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize params.\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 237\u001b[0m model \u001b[38;5;241m=\u001b[39m model \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# check context_window\u001b[39;00m\n\u001b[1;32m    242\u001b[0m config_dict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "File \u001b[0;32m~/vishak/miniconda3/envs/lora/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/vishak/miniconda3/envs/lora/lib/python3.10/site-packages/transformers/modeling_utils.py:4225\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4216\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4218\u001b[0m     (\n\u001b[1;32m   4219\u001b[0m         model,\n\u001b[1;32m   4220\u001b[0m         missing_keys,\n\u001b[1;32m   4221\u001b[0m         unexpected_keys,\n\u001b[1;32m   4222\u001b[0m         mismatched_keys,\n\u001b[1;32m   4223\u001b[0m         offload_index,\n\u001b[1;32m   4224\u001b[0m         error_msgs,\n\u001b[0;32m-> 4225\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4232\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4233\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4236\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4237\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4245\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4246\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/vishak/miniconda3/envs/lora/lib/python3.10/site-packages/transformers/modeling_utils.py:4728\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4724\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4725\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4726\u001b[0m                 )\n\u001b[1;32m   4727\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4728\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4729\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4730\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4731\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4732\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4733\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4734\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4735\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4736\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4737\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4738\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4739\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4740\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4741\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4742\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4743\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4744\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4746\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m~/vishak/miniconda3/envs/lora/lib/python3.10/site-packages/transformers/modeling_utils.py:993\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    990\u001b[0m         param_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 993\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    995\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m~/vishak/miniconda3/envs/lora/lib/python3.10/site-packages/accelerate/utils/modeling.py:329\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    327\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 329\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/vishak/miniconda3/envs/lora/lib/python3.10/site-packages/torch/cuda/__init__.py:319\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    318\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 319\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    323\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "# Step 1: Set up the HuggingFace LLM\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"aaditya/OpenBioLLM-Llama3-8B\",\n",
    "    tokenizer_name=\"aaditya/OpenBioLLM-Llama3-8B\",\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "Settings.llm=llm\n",
    "Settings.embed_model=\"local:BAAI/bge-small-en-v1.5\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load the document (assuming it's stored in a text file)\n",
    "document_path = \"/raid/ganesh/vishak/ashutosh/temp/temp1\"  # Path to your document\n",
    "reader = SimpleDirectoryReader(input_dir=document_path)\n",
    "documents = reader.load_data()\n",
    "\n",
    "nodes = Settings.node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# initialize storage context (by default it's in-memory)\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
    "keyword_index = SimpleKeywordTableIndex(nodes, storage_context=storage_context)\n",
    "\n",
    "retriever = vector_index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(retrieved_nodes, query_str, qa_prompt, llm):\n",
    "    context_str = \"\\n\\n\".join([r.get_content() for r in retrieved_nodes])\n",
    "    fmt_qa_prompt = qa_prompt.format(\n",
    "        context_str=context_str, query_str=query_str\n",
    "    )\n",
    "    response = llm.complete(fmt_qa_prompt)\n",
    "    return str(response), fmt_qa_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  Yes\n"
     ]
    }
   ],
   "source": [
    "Question = \"Was the patient considered for curative intent treatment?\"\n",
    "\n",
    "retrieved_nodes = retriever.retrieve(Question)\n",
    "\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    \"\"\"\\\n",
    "    Context information is below.\n",
    "    I want you to act like a breast cancer expert/doctor.\n",
    "    I will give you summary of a breast cancer patient's stay in the hospital, you will evaluate it and answer a set of questions as 'Yes' or 'No' only.\n",
    "    ---------------------\n",
    "    {context_str}\n",
    "    ---------------------\n",
    "    Given the context information and not prior knowledge, answer the query.\n",
    "    Query: {query_str}\n",
    "    Answer: \\\n",
    "    \"\"\"\n",
    ")\n",
    "response_text= generate_response(retrieved_nodes, Question, qa_prompt, llm )\n",
    "print(\"\\n\\n\\n\", response_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = \"\"\"\n",
    "    Was the patient considered for curative intent treatment?\n",
    "    Did the patient undergo a Mammogram (MMG)?\n",
    "    Did the patient undergo a Ultrasound (USG)?\n",
    "    Did the patient undergo a physical examination of both breasts?\n",
    "    Did the patient undergo a physical examination of axilla?\n",
    "    Is the patient undergoing upfront surgery?\n",
    "    Does the patient have a pre-operatively confirmed histological or cytological diagnosis?\n",
    "    Was the patient considered for curative intent treatment?\n",
    "    Did the patient undergo image-guided axillary staging?\n",
    "    Was the patient considered for curative intent treatment?\n",
    "    Does the biopsy report of the patient include Prognostic factors?\n",
    "    Does the biopsy report of the patient include predictive factors?\n",
    "    Is the breast cancer grade recorded in the biopsy report of the patient?\n",
    "    Is ER (Estrogen Receptor status) recorded in the biopsy report of the patient?\n",
    "    Is PR (Progesterone Receptor status) recorded in the biopsy report of the patient?\n",
    "    Is HER2Neu status recorded in the biopsy report of the patient?\n",
    "    Is the patient undergoing upfront systemic therapy?\n",
    "    Did the patient have a core biopsy?\n",
    "    Is the patient undergoing upfront systemic therapy?\n",
    "    Does the patient have ER (Estrogen Receptor status) recorded?\n",
    "    Does the patient have PR (Progesterone Receptor status) recorded?\n",
    "    Does the patient have HER2Neu status recorded?\n",
    "    Was the patient considered for curative intent treatment?\n",
    "    What is the time interval from the date of the first diagnostic examination to the date of surgery?\n",
    "    What is the time interval from the date of the first diagnostic examination to the start of systemic therapy?\n",
    "    Is the time interval from the date of the first diagnostic examination to the date of surgery/start of systemic therapy less than 6 weeks?\n",
    "    Was the decision to consider the patient for curative intent treatment taken by a multidisciplinary team (Tumor board)?\n",
    "    Is the patient diagnosed with stage I breast cancer?\n",
    "    Is the patient diagnosed with primary operable stage II breast cancer?\n",
    "    Did the patient not undergo a baseline USG (Ultrasound) as a staging test?\n",
    "    Did the patient not undergo a baseline bone scan as a staging test?\n",
    "    Is the patient diagnosed with stage III breast cancer?\n",
    "    Did the patient undergo a baseline USG of the abdomen (USG abd)?\n",
    "    Did the patient undergo a baseline Chest X-Ray (CXR)?\n",
    "    Did the patient undergo baseline Liver Function Tests (LFT)?\n",
    "    Did the patient undergo a baseline bone scan?\n",
    "    Alternatively, did the patient undergo a PET Scan?\n",
    "    Alternatively, did the patient undergo a CT Thorax with abdomen?\n",
    "    Was the patient planned for Breast Conserving Surgery (BCS)?\n",
    "    Did the patient receive tumor localization before pre-operative systemic therapy?\n",
    "    Is the patient at high risk for breast cancer?\n",
    "    Does the patient have triple negative breast cancer?\n",
    "    Did the patient receive genetic counselling?\n",
    "    Did the patient get genetic testing before starting adjuvant therapy with olaparib?\n",
    "    Does the patient have HER2 Neu scored as 2+?\n",
    "    Was the HER2 Neu 2+ status confirmed by FISH (Fluorescence In Situ Hybridization)?\n",
    "    Did the breast cancer patient have Pre-Anesthetic Checkup (PAC) documented prior to surgery?\n",
    "    Did the patient with breast cancer have a mammogram at the time of diagnosis?\n",
    "    Was the mammogram report of the patient documented using the BIRADS (Breast Imaging-Reporting and Data System) standard?\n",
    "    Is the patient receiving antiHer2 therapy?\n",
    "    Is the antiHer2 therapy being used as neoadjuvant therapy?\n",
    "    Is the antiHer2 therapy being used as adjuvant therapy?\n",
    "    Is the patient receiving at least 12 weeks of antiHer2 therapy?\n",
    "    Is the patient node positive for breast cancer?\n",
    "    Was the node-positive patient advised to undergo chemotherapy?\n",
    "    Is the patient hormone receptor (ER/PR) positive?\n",
    "    Did the hormone receptor (ER/PR) positive patient receive tamoxifen treatment?\n",
    "    Did the hormone receptor (ER/PR) positive patient receive aromatase inhibitor treatment?\n",
    "    Has the patient started chemotherapy?\n",
    "    Prior to starting chemotherapy, was the Complete Blood Count (CBC) documented?\n",
    "    Prior to starting chemotherapy, was the Renal Function Test (RFT) documented?\n",
    "    Prior to starting chemotherapy, was the Liver Function Test (LFT) documented?\n",
    "    Prior to starting chemotherapy, was the Random Blood Sugar (RBS) documented?\n",
    "    Has the patient started anthracycline and/or trastuzumab-based chemotherapy?\n",
    "    Prior to starting anthracycline-based chemotherapy, was an ECHO or MUGA scan report documented?\n",
    "    Prior to starting trastuzumab-based chemotherapy, was an ECHO or MUGA scan report documented?\n",
    "    During neoadjuvant chemotherapy, was there documented reassessment of the lump status at each visit?\n",
    "    Did the patient receiving chemotherapy have documented toxicities to the chemotherapy?\n",
    "    Was the patient diagnosed with stage I invasive breast cancer and undergone a single operation?\n",
    "    Was the patient diagnosed with stage II invasive breast cancer and undergone a single operation?\n",
    "    Was the patient diagnosed with stage III invasive breast cancer and undergone a single operation?\n",
    "    Was the patient diagnosed with in-situ breast cancer and undergone a single surgery?\n",
    "    Was the patient diagnosed with stage I breast cancer?\n",
    "    Alternatively, was the patient diagnosed with stage II breast cancer?\n",
    "    Was the patient's axilla clinically assessed as negative?\n",
    "    Did the patient receive any pre-surgery therapy (PST)?\n",
    "    If the patient received no PST, did they undergo a Sentinel Lymph Node Biopsy (SLNB)?\n",
    "    Did the patient undergo Sentinel Lymph Node Biopsy (SLNB)?\n",
    "    Were less than 5 nodes removed during the SLNB?\n",
    "    Was the patient diagnosed with Ductal Carcinoma In Situ (DCIS)?\n",
    "    Did the patient undergoing surgery for DCIS not undergo axillary clearance?\n",
    "    Was the patient diagnosed with invasive/infiltrating breast cancer?\n",
    "    Did the patient undergo breast conservation surgery?\n",
    "    Did the patient receive adjuvant radiotherapy after breast conservation surgery?\n",
    "    Does the patient have involvement of axillary lymph nodes (≥ pN2a)?\n",
    "    Did the patient undergo a mastectomy?\n",
    "    Did the patient receive post-mastectomy radiation therapy to the chest wall and supraclavicular fossa (SCF)?\n",
    "    Was the patient diagnosed with high-grade breast cancer?\n",
    "    Does the patient have focal positive margins?\n",
    "    Is the patient aged 50 years or younger?\n",
    "    Did the patient receive a lumpectomy?\n",
    "    Did the patient receive a lumpectomy cavity boost?\n",
    "    Was the internal mammary node diagnosed with cancer in the patient?\n",
    "    Did the patient receive radiotherapy to the internal mammary node?\n",
    "    Was the patient clinically diagnosed as node positive?\n",
    "    Did the patient undergo neoadjuvant chemotherapy?\n",
    "    Did the patient receive adjuvant radiotherapy?\n",
    "    Is the patient being treated with curative intent?\n",
    "    Did the treatment start within 6 weeks of diagnosis?\n",
    "    Did the patient start radiation therapy (RT) within 7 months after surgery?\n",
    "    Did the patient start chemotherapy within 12 weeks post-surgery?\n",
    "    Was the breast cancer patient followed up within 10 to 12 weeks after completing treatment?\n",
    "    Did the patient start systemic (adjuvant/neoadjuvant) treatment?\n",
    "    Did the patient complete systemic therapy within 8 months of starting?\n",
    "    Note: This excludes patients on anti-HER2 and hormonal therapy.\n",
    "\"\"\"\n",
    "questions = [ques.strip() for ques in questions.strip().split(\"\\n\")]\n",
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Was the patient considered for curative intent treatment?',\n",
       " 'Did the patient undergo a Mammogram (MMG)?',\n",
       " 'Did the patient undergo a Ultrasound (USG)?',\n",
       " 'Did the patient undergo a physical examination of both breasts?',\n",
       " 'Did the patient undergo a physical examination of axilla?',\n",
       " 'Is the patient undergoing upfront surgery?',\n",
       " 'Does the patient have a pre-operatively confirmed histological or cytological diagnosis?',\n",
       " 'Was the patient considered for curative intent treatment?',\n",
       " 'Did the patient undergo image-guided axillary staging?',\n",
       " 'Was the patient considered for curative intent treatment?',\n",
       " 'Does the biopsy report of the patient include Prognostic factors?',\n",
       " 'Does the biopsy report of the patient include predictive factors?',\n",
       " 'Is the breast cancer grade recorded in the biopsy report of the patient?',\n",
       " 'Is ER (Estrogen Receptor status) recorded in the biopsy report of the patient?',\n",
       " 'Is PR (Progesterone Receptor status) recorded in the biopsy report of the patient?',\n",
       " 'Is HER2Neu status recorded in the biopsy report of the patient?',\n",
       " 'Is the patient undergoing upfront systemic therapy?',\n",
       " 'Did the patient have a core biopsy?',\n",
       " 'Is the patient undergoing upfront systemic therapy?',\n",
       " 'Does the patient have ER (Estrogen Receptor status) recorded?',\n",
       " 'Does the patient have PR (Progesterone Receptor status) recorded?',\n",
       " 'Does the patient have HER2Neu status recorded?',\n",
       " 'Was the patient considered for curative intent treatment?',\n",
       " 'What is the time interval from the date of the first diagnostic examination to the date of surgery?',\n",
       " 'What is the time interval from the date of the first diagnostic examination to the start of systemic therapy?',\n",
       " 'Is the time interval from the date of the first diagnostic examination to the date of surgery/start of systemic therapy less than 6 weeks?',\n",
       " 'Was the decision to consider the patient for curative intent treatment taken by a multidisciplinary team (Tumor board)?',\n",
       " 'Is the patient diagnosed with stage I breast cancer?',\n",
       " 'Is the patient diagnosed with primary operable stage II breast cancer?',\n",
       " 'Did the patient not undergo a baseline USG (Ultrasound) as a staging test?',\n",
       " 'Did the patient not undergo a baseline bone scan as a staging test?',\n",
       " 'Is the patient diagnosed with stage III breast cancer?',\n",
       " 'Did the patient undergo a baseline USG of the abdomen (USG abd)?',\n",
       " 'Did the patient undergo a baseline Chest X-Ray (CXR)?',\n",
       " 'Did the patient undergo baseline Liver Function Tests (LFT)?',\n",
       " 'Did the patient undergo a baseline bone scan?',\n",
       " 'Alternatively, did the patient undergo a PET Scan?',\n",
       " 'Alternatively, did the patient undergo a CT Thorax with abdomen?',\n",
       " 'Was the patient planned for Breast Conserving Surgery (BCS)?',\n",
       " 'Did the patient receive tumor localization before pre-operative systemic therapy?',\n",
       " 'Is the patient at high risk for breast cancer?',\n",
       " 'Does the patient have triple negative breast cancer?',\n",
       " 'Did the patient receive genetic counselling?',\n",
       " 'Did the patient get genetic testing before starting adjuvant therapy with olaparib?',\n",
       " 'Does the patient have HER2 Neu scored as 2+?',\n",
       " 'Was the HER2 Neu 2+ status confirmed by FISH (Fluorescence In Situ Hybridization)?',\n",
       " 'Did the breast cancer patient have Pre-Anesthetic Checkup (PAC) documented prior to surgery?',\n",
       " 'Did the patient with breast cancer have a mammogram at the time of diagnosis?',\n",
       " 'Was the mammogram report of the patient documented using the BIRADS (Breast Imaging-Reporting and Data System) standard?',\n",
       " 'Is the patient receiving antiHer2 therapy?',\n",
       " 'Is the antiHer2 therapy being used as neoadjuvant therapy?',\n",
       " 'Is the antiHer2 therapy being used as adjuvant therapy?',\n",
       " 'Is the patient receiving at least 12 weeks of antiHer2 therapy?',\n",
       " 'Is the patient node positive for breast cancer?',\n",
       " 'Was the node-positive patient advised to undergo chemotherapy?',\n",
       " 'Is the patient hormone receptor (ER/PR) positive?',\n",
       " 'Did the hormone receptor (ER/PR) positive patient receive tamoxifen treatment?',\n",
       " 'Did the hormone receptor (ER/PR) positive patient receive aromatase inhibitor treatment?',\n",
       " 'Has the patient started chemotherapy?',\n",
       " 'Prior to starting chemotherapy, was the Complete Blood Count (CBC) documented?',\n",
       " 'Prior to starting chemotherapy, was the Renal Function Test (RFT) documented?',\n",
       " 'Prior to starting chemotherapy, was the Liver Function Test (LFT) documented?',\n",
       " 'Prior to starting chemotherapy, was the Random Blood Sugar (RBS) documented?',\n",
       " 'Has the patient started anthracycline and/or trastuzumab-based chemotherapy?',\n",
       " 'Prior to starting anthracycline-based chemotherapy, was an ECHO or MUGA scan report documented?',\n",
       " 'Prior to starting trastuzumab-based chemotherapy, was an ECHO or MUGA scan report documented?',\n",
       " 'During neoadjuvant chemotherapy, was there documented reassessment of the lump status at each visit?',\n",
       " 'Did the patient receiving chemotherapy have documented toxicities to the chemotherapy?',\n",
       " 'Was the patient diagnosed with stage I invasive breast cancer and undergone a single operation?',\n",
       " 'Was the patient diagnosed with stage II invasive breast cancer and undergone a single operation?',\n",
       " 'Was the patient diagnosed with stage III invasive breast cancer and undergone a single operation?',\n",
       " 'Was the patient diagnosed with in-situ breast cancer and undergone a single surgery?',\n",
       " 'Was the patient diagnosed with stage I breast cancer?',\n",
       " 'Alternatively, was the patient diagnosed with stage II breast cancer?',\n",
       " \"Was the patient's axilla clinically assessed as negative?\",\n",
       " 'Did the patient receive any pre-surgery therapy (PST)?',\n",
       " 'If the patient received no PST, did they undergo a Sentinel Lymph Node Biopsy (SLNB)?',\n",
       " 'Did the patient undergo Sentinel Lymph Node Biopsy (SLNB)?',\n",
       " 'Were less than 5 nodes removed during the SLNB?',\n",
       " 'Was the patient diagnosed with Ductal Carcinoma In Situ (DCIS)?',\n",
       " 'Did the patient undergoing surgery for DCIS not undergo axillary clearance?',\n",
       " 'Was the patient diagnosed with invasive/infiltrating breast cancer?',\n",
       " 'Did the patient undergo breast conservation surgery?',\n",
       " 'Did the patient receive adjuvant radiotherapy after breast conservation surgery?',\n",
       " 'Does the patient have involvement of axillary lymph nodes (≥ pN2a)?',\n",
       " 'Did the patient undergo a mastectomy?',\n",
       " 'Did the patient receive post-mastectomy radiation therapy to the chest wall and supraclavicular fossa (SCF)?',\n",
       " 'Was the patient diagnosed with high-grade breast cancer?',\n",
       " 'Does the patient have focal positive margins?',\n",
       " 'Is the patient aged 50 years or younger?',\n",
       " 'Did the patient receive a lumpectomy?',\n",
       " 'Did the patient receive a lumpectomy cavity boost?',\n",
       " 'Was the internal mammary node diagnosed with cancer in the patient?',\n",
       " 'Did the patient receive radiotherapy to the internal mammary node?',\n",
       " 'Was the patient clinically diagnosed as node positive?',\n",
       " 'Did the patient undergo neoadjuvant chemotherapy?',\n",
       " 'Did the patient receive adjuvant radiotherapy?',\n",
       " 'Is the patient being treated with curative intent?',\n",
       " 'Did the treatment start within 6 weeks of diagnosis?',\n",
       " 'Did the patient start radiation therapy (RT) within 7 months after surgery?',\n",
       " 'Did the patient start chemotherapy within 12 weeks post-surgery?',\n",
       " 'Was the breast cancer patient followed up within 10 to 12 weeks after completing treatment?',\n",
       " 'Did the patient start systemic (adjuvant/neoadjuvant) treatment?',\n",
       " 'Did the patient complete systemic therapy within 8 months of starting?',\n",
       " 'Note: This excludes patients on anti-HER2 and hormonal therapy.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Query:   0%|          | 0/105 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:   1%|          | 1/105 [00:00<00:25,  4.11it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:   2%|▏         | 2/105 [00:00<00:48,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:   3%|▎         | 3/105 [00:01<00:56,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:   4%|▍         | 4/105 [00:01<00:42,  2.38it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:   5%|▍         | 5/105 [00:01<00:34,  2.88it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:   6%|▌         | 6/105 [00:02<00:30,  3.30it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:   7%|▋         | 7/105 [00:02<00:26,  3.64it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:   8%|▊         | 8/105 [00:02<00:24,  3.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:   9%|▊         | 9/105 [00:02<00:23,  4.06it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  10%|▉         | 10/105 [00:03<00:24,  3.85it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  10%|█         | 11/105 [00:03<00:34,  2.73it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  11%|█▏        | 12/105 [00:03<00:29,  3.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  12%|█▏        | 13/105 [00:04<00:26,  3.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  13%|█▎        | 14/105 [00:04<00:33,  2.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  14%|█▍        | 15/105 [00:05<00:37,  2.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  15%|█▌        | 16/105 [00:07<01:19,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  16%|█▌        | 17/105 [00:07<01:00,  1.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  17%|█▋        | 18/105 [00:07<00:47,  1.83it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  18%|█▊        | 19/105 [00:07<00:38,  2.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  19%|█▉        | 20/105 [00:08<00:31,  2.66it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  20%|██        | 21/105 [00:08<00:27,  3.05it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  21%|██        | 22/105 [00:08<00:33,  2.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  22%|██▏       | 23/105 [00:09<00:36,  2.22it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  23%|██▎       | 24/105 [00:10<00:41,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  24%|██▍       | 25/105 [00:11<00:50,  1.58it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  25%|██▍       | 26/105 [00:11<00:39,  1.98it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  26%|██▌       | 27/105 [00:11<00:32,  2.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  27%|██▋       | 28/105 [00:11<00:27,  2.81it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  28%|██▊       | 29/105 [00:11<00:23,  3.19it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  29%|██▊       | 30/105 [00:12<00:31,  2.36it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  30%|██▉       | 31/105 [00:13<00:37,  1.99it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  30%|███       | 32/105 [00:13<00:37,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  31%|███▏      | 33/105 [00:14<00:30,  2.34it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  32%|███▏      | 34/105 [00:14<00:25,  2.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  33%|███▎      | 35/105 [00:14<00:22,  3.14it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  34%|███▍      | 36/105 [00:14<00:19,  3.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  35%|███▌      | 37/105 [00:15<00:24,  2.82it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  36%|███▌      | 38/105 [00:15<00:28,  2.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  37%|███▋      | 39/105 [00:15<00:23,  2.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  38%|███▊      | 40/105 [00:16<00:20,  3.18it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  39%|███▉      | 41/105 [00:16<00:24,  2.60it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  40%|████      | 42/105 [00:16<00:20,  3.01it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  41%|████      | 43/105 [00:17<00:18,  3.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  42%|████▏     | 44/105 [00:17<00:27,  2.18it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  43%|████▎     | 45/105 [00:18<00:23,  2.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  44%|████▍     | 46/105 [00:18<00:19,  3.00it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  45%|████▍     | 47/105 [00:18<00:17,  3.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  46%|████▌     | 48/105 [00:18<00:15,  3.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  47%|████▋     | 49/105 [00:19<00:14,  3.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  48%|████▊     | 50/105 [00:19<00:12,  4.24it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  49%|████▊     | 51/105 [00:19<00:12,  4.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  50%|████▉     | 52/105 [00:19<00:11,  4.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  50%|█████     | 53/105 [00:19<00:11,  4.65it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  51%|█████▏    | 54/105 [00:20<00:10,  4.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  52%|█████▏    | 55/105 [00:20<00:10,  4.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  53%|█████▎    | 56/105 [00:20<00:10,  4.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  54%|█████▍    | 57/105 [00:20<00:10,  4.73it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  55%|█████▌    | 58/105 [00:20<00:09,  4.74it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  56%|█████▌    | 59/105 [00:21<00:09,  4.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  57%|█████▋    | 60/105 [00:21<00:09,  4.72it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  58%|█████▊    | 61/105 [00:21<00:09,  4.74it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  59%|█████▉    | 62/105 [00:21<00:09,  4.70it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  60%|██████    | 63/105 [00:21<00:08,  4.70it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  61%|██████    | 64/105 [00:30<01:45,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  62%|██████▏   | 65/105 [00:30<01:23,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  63%|██████▎   | 66/105 [00:31<00:59,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  64%|██████▍   | 67/105 [00:31<00:42,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  65%|██████▍   | 68/105 [00:31<00:31,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  66%|██████▌   | 69/105 [00:31<00:23,  1.52it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  67%|██████▋   | 70/105 [00:32<00:18,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  68%|██████▊   | 71/105 [00:32<00:14,  2.33it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  69%|██████▊   | 72/105 [00:32<00:11,  2.75it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  70%|██████▉   | 73/105 [00:32<00:10,  3.15it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  70%|███████   | 74/105 [00:32<00:08,  3.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  71%|███████▏  | 75/105 [00:33<00:07,  3.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  72%|███████▏  | 76/105 [00:33<00:10,  2.70it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  73%|███████▎  | 77/105 [00:33<00:09,  3.09it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  74%|███████▍  | 78/105 [00:34<00:07,  3.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  75%|███████▌  | 79/105 [00:34<00:10,  2.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  76%|███████▌  | 80/105 [00:34<00:08,  3.01it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  77%|███████▋  | 81/105 [00:35<00:09,  2.56it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  78%|███████▊  | 82/105 [00:35<00:07,  2.97it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  79%|███████▉  | 83/105 [00:35<00:06,  3.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  80%|████████  | 84/105 [00:36<00:05,  3.70it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  81%|████████  | 85/105 [00:36<00:05,  3.95it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  82%|████████▏ | 86/105 [00:36<00:04,  4.17it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  83%|████████▎ | 87/105 [00:37<00:07,  2.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  84%|████████▍ | 88/105 [00:37<00:05,  2.87it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  85%|████████▍ | 89/105 [00:37<00:04,  3.24it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  86%|████████▌ | 90/105 [00:37<00:04,  3.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  87%|████████▋ | 91/105 [00:38<00:03,  3.78it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  88%|████████▊ | 92/105 [00:38<00:03,  4.00it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  89%|████████▊ | 93/105 [00:38<00:02,  4.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  90%|████████▉ | 94/105 [00:38<00:02,  4.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  90%|█████████ | 95/105 [00:39<00:02,  4.32it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  91%|█████████▏| 96/105 [00:39<00:02,  4.06it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  92%|█████████▏| 97/105 [00:39<00:01,  4.18it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  93%|█████████▎| 98/105 [00:39<00:01,  4.26it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  94%|█████████▍| 99/105 [00:39<00:01,  4.36it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  95%|█████████▌| 100/105 [00:40<00:01,  4.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  96%|█████████▌| 101/105 [00:40<00:00,  4.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  97%|█████████▋| 102/105 [00:40<00:00,  4.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  98%|█████████▊| 103/105 [00:40<00:00,  4.56it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query:  99%|█████████▉| 104/105 [00:41<00:00,  4.58it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing Query: 100%|██████████| 105/105 [00:41<00:00,  2.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "outputs=  []\n",
    "for i in tqdm(range(len(questions)), \"Processing Query\"):\n",
    "    query = questions[i]\n",
    "    retrieved_nodes = retriever.retrieve(query)\n",
    "    qa_prompt = PromptTemplate(\n",
    "        \"\"\"\\\n",
    "        Context information is below.\n",
    "        I want you to act like a breast cancer expert/doctor.\n",
    "        I will give you summary of a breast cancer patient's stay in the hospital, you will evaluate it and answer a set of questions as 'Yes' or 'No' only.\n",
    "        ---------------------\n",
    "        {context_str}\n",
    "        ---------------------\n",
    "        Given the context information and not prior knowledge, answer the query.\n",
    "        Query: {query_str}\n",
    "        Answer: \\\n",
    "        \"\"\"\n",
    "    )\n",
    "    response_text= generate_response(retrieved_nodes, query, qa_prompt, llm)\n",
    "    outputs.append(response_text)\n",
    "    with open(\"output.txt\", \"a\") as f:\n",
    "        f.write(response_text[0]+\"\\n\")\n",
    "\n",
    "\n",
    "with open(\"log.txt\", \"a\") as f:\n",
    "    for i in range(len(outputs)):\n",
    "        f.write(outputs[i][0] + \"\\n\" + outputs[i][1]+ \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Lora finetuned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (0.44.1)\n",
      "Requirement already satisfied: torch in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from bitsandbytes) (2.5.1)\n",
      "Requirement already satisfied: numpy in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from torch->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from torch->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from torch->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from torch->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from torch->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from torch->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from torch->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from torch->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ganesh/vishak/miniconda3/envs/lora/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m base_model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m      5\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      6\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m      7\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      8\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16 \n\u001b[1;32m      9\u001b[0m     )\n\u001b[0;32m---> 11\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Mistral, same as before \u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m eval_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model_id, add_bos_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n",
      "File \u001b[0;32m~/vishak/miniconda3/envs/lora/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/vishak/miniconda3/envs/lora/lib/python3.10/site-packages/transformers/modeling_utils.py:3657\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3654\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3657\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3659\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3660\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3661\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m~/vishak/miniconda3/envs/lora/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:82\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n\u001b[1;32m     81\u001b[0m bnb_multibackend_is_enabled \u001b[38;5;241m=\u001b[39m is_bitsandbytes_multi_backend_available()\n\u001b[0;32m---> 82\u001b[0m \u001b[43mvalidate_bnb_backend_availability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_tf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_flax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m sure the weights are in PyTorch format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m     )\n",
      "File \u001b[0;32m~/vishak/miniconda3/envs/lora/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py:558\u001b[0m, in \u001b[0;36mvalidate_bnb_backend_availability\u001b[0;34m(raise_exception)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bitsandbytes_multi_backend_available():\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _validate_bnb_multi_backend_availability(raise_exception)\n\u001b[0;32m--> 558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_validate_bnb_cuda_backend_availability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vishak/miniconda3/envs/lora/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py:536\u001b[0m, in \u001b[0;36m_validate_bnb_cuda_backend_availability\u001b[0;34m(raise_exception)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_exception:\n\u001b[1;32m    535\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(log_msg)\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(log_msg)\n\u001b[1;32m    538\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning(log_msg)\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\" \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, \n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 \n",
    "    )\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id, # Mistral, same as before \n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\", \n",
    "    trust_remote_code=True)\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"ZEECO1/CancerLLM-Mistral7b/checkpoint-500\")\n",
    "\n",
    "eval_prompt = \" what are the drugs against lung cancer: # \" \n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval() \n",
    "with torch.no_grad(): \n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
